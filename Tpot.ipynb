{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b1393d-461d-4cf2-9174-d03a32f8c3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.4.1.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting update_checker\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting stopit\n",
      "  Using cached stopit-1.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: xgboost in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from deap) (1.26.2)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from update_checker) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from xgboost) (1.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update_checker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update_checker) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update_checker) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update_checker) (2023.11.17)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: deap\n",
      "  Building wheel for deap (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deap: filename=deap-1.4.1-cp310-cp310-macosx_14_0_arm64.whl size=104428 sha256=ff4034d3efb67d65bddd2f966a87bf7e817d51734a6ad4265037758b8de5e74c\n",
      "  Stored in directory: /Users/youssefelmaaoua/Library/Caches/pip/wheels/0e/f2/e3/f1bd8b40cf7eefa319d5a27aa9ba3cc588e6ada69b5919590d\n",
      "Successfully built deap\n",
      "Installing collected packages: stopit, tqdm, deap, update_checker\n",
      "Successfully installed deap-1.4.1 stopit-1.1.2 tqdm-4.66.1 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deap update_checker tqdm stopit xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c0245d-fdce-4e93-a4c6-eb230d99d290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.3.2)\n",
      "Requirement already satisfied: deap>=1.2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.4.1)\n",
      "Requirement already satisfied: update-checker>=0.16 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (4.66.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (2.1.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (1.3.2)\n",
      "Requirement already satisfied: xgboost>=1.1.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from tpot) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas>=0.24.2->tpot) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas>=0.24.2->tpot) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from scikit-learn>=0.22.0->tpot) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from update-checker>=0.16->tpot) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install tpot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c662fdf-1460-45f8-8def-2f118defe1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torch-2.1.1-cp310-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2196b2ea-127d-4436-9f4a-67bd439c05c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: joblib in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (1.3.2)\n",
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pytorch \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/youssefelmaaoua/.pyenv/versions/3.10.6/envs/Deep_fake_voice_recognition/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/2q/1xbtf4l11ygbnx9cd6jf8znw0000gn/T/pip-build-env-vbk0vpk1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 404, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/2q/1xbtf4l11ygbnx9cd6jf8znw0000gn/T/pip-build-env-vbk0vpk1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 389, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/2q/1xbtf4l11ygbnx9cd6jf8znw0000gn/T/pip-build-env-vbk0vpk1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 480, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/2q/1xbtf4l11ygbnx9cd6jf8znw0000gn/T/pip-build-env-vbk0vpk1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pytorch\n",
      "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy scikit-learn pandas joblib pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc5a6e5f-1555-4531-b49e-b18c640a98c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bdad38-2541-4f3a-b015-dacc2be81ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1010854-c348-4c9f-a1f4-2bf16279a9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7af5c08a-d3d4-4305-9fcb-29f84dc18d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_test(complete_df):\n",
    "    #split, process and return  X_train, X_test, y_train, y_test\n",
    "    df_train, df_test = train_test_split(complete_df, test_size=0.3,stratify=complete_df['LABEL'])\n",
    "\n",
    "    count = df_train['LABEL'].value_counts()\n",
    "    count[count.isin([min(count)])].index[0]\n",
    "    under_sampled = df_train[df_train['LABEL']==count[count.isin([max(count)])].index[0]].sample(min(count))\n",
    "    balanced_df = pd.concat([under_sampled, df_train[df_train['LABEL']==count[count.isin([min(count)])].index[0]]], axis=0)\n",
    "    X_train = balanced_df[balanced_df.columns[0:-1]]\n",
    "    X_test = df_test[balanced_df.columns[0:-1]]\n",
    "    y_label = balanced_df['LABEL']\n",
    "    y_label_test = df_test['LABEL']\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_label)\n",
    "    # le.classes_\n",
    "    y_train = le.transform(y_label)\n",
    "    y_test = le.transform(y_label_test)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8374ad72-4007-4def-8001-1b94235b6a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(svm_XGB_best2):\n",
    "    #test model with our tests\n",
    "\n",
    "    #read tests\n",
    "    new_test_taylor_r = pd.read_csv('Data/extracted_segment_10_extracted_segment_Taylor Swift Talks Record-Breaking Midnights Album, Music Video Cameos and Easter Eggs.csv')\n",
    "    new_test_bidenf = pd.read_csv('Data/audio_features_biden_AI.csv')\n",
    "    new_test_bidenr = pd.read_csv('Data/audio_features_biden.csv')\n",
    "    new_test_el = pd.read_csv('Data/Elise.csv')\n",
    "    new_test_yr1 = pd.read_csv('Data/Youssef.csv')\n",
    "    new_test_yr2 = pd.read_csv('Data/Youssef_2.csv')\n",
    "    new_test_yf1 = pd.read_csv('Data/Youssef_Eric Cartman.csv')\n",
    "    new_test_yf2 = pd.read_csv('Data/Youssef_2_Female.csv')\n",
    "    new_test_yf3  = pd.read_csv('Data/Youssef_2_Male Reggaeton.csv')\n",
    "    new_test_MF_f  = pd.read_csv('Data/Morgan Freeman_fake.csv')\n",
    "    new_test_MF_r  = pd.read_csv('Data/Morgan Freeman_real.csv')\n",
    "    #make y_true\n",
    "    y_real = np.ones((1000,))\n",
    "    y_fake = np.zeros((1000,))\n",
    "\n",
    "    #predict and get accuray score\n",
    "    pred = svm_XGB_best2.predict(new_test_taylor_r)\n",
    "    taylor_r = accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Taylor : ' , taylor_r)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_bidenf[new_test_bidenf.columns[0:-2]])\n",
    "    bidenf = accuracy_score(y_fake[:len(pred)],pred)\n",
    "    print('Fake Biden : ' , bidenf)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_bidenr[new_test_bidenf.columns[0:-2]])\n",
    "    bidenr=accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Biden : ' , bidenr)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_el)\n",
    "    eliser=accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Elise : ' , eliser)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_yr1)\n",
    "    youssr1 = accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Youss_1 : ' , youssr1)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_yr2)\n",
    "    youssr2 = accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Youss_2 : ' , youssr2)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_yf1)\n",
    "    youssf1 = accuracy_score(y_fake[:len(pred)],pred)\n",
    "    print('Fake Youss_1 : ' , youssf1)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_yf2)\n",
    "    youssf2 = accuracy_score(y_fake[:len(pred)],pred)\n",
    "    print('Fake Youss_2 : ' , youssf2)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_yf3)\n",
    "    youssf3 = accuracy_score(y_fake[:len(pred)],pred)\n",
    "    print('Fake Youss_3 : ' , youssf3)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_MF_f)\n",
    "    morganf = accuracy_score(y_fake[:len(pred)],pred)\n",
    "    print('Fake Morgan : ' , morganf)\n",
    "\n",
    "    pred = svm_XGB_best2.predict(new_test_MF_r)\n",
    "    morganr = accuracy_score(y_real[:len(pred)],pred)\n",
    "    print('Real Morgan : ' , morganr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a2f2287-481f-4238-9c5f-6146acd9a435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier(n_jobs=-1,warm_start=True,verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70430e1d-ecf8-4521-b958-22461fcb0e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "face661a-22fe-43b0-9996-fab2c7190680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=f19050c02cb2f96bf347631992ddc45e74140791c335109b9e1e318930b47ebd\n",
      "  Stored in directory: /Users/youssefelmaaoua/Library/Caches/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6625b7fc-06b1-4b0d-a29a-18dbbf3b0af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.getAvailable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a005c1d-f136-4d45-9f9f-cb5df21adbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6693c612-d8e0-4501-be32-c30f01fb3279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/youssefelmaaoua/code/Andreanorm/Deep_fake_voice_recognition'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c833e083-3808-4538-8778-1be6a57c16de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/df_asvc_without_LA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a81ee6e-3fb8-4e72-a803-f118f6a9b4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "769fa51e-9d07-4e66-9549-808f91c554d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb7626304584ee6a6d9a3a86ca86696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, n_jobs=1, subsample=0.2, verbosity=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTClassifier(n_jobs=7, verbosity=2, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TPOTClassifier</label><div class=\"sk-toggleable__content\"><pre>TPOTClassifier(n_jobs=7, verbosity=2, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TPOTClassifier(n_jobs=7, verbosity=2, warm_start=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "477c215a-a2c5-4c19-9aa9-1e3b25cdd7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTClassifier(n_jobs=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TPOTClassifier</label><div class=\"sk-toggleable__content\"><pre>TPOTClassifier(n_jobs=7)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TPOTClassifier(n_jobs=7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c04cad6-79bc-45f4-8312-08fbe7b9d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6663023226928332\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_optimizer.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa50be94-ba47-48fd-ab2c-2bf64e6dbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.export('tpot_exported_pipeline_withoutLA_v2.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a25b765b-66d8-4b76-b656-9a998c419c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./test_tpot_notebook.pkl','wb') as f:\n",
    "    pickle.dump(pipeline_optimizer.fitted_pipeline_, f)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
